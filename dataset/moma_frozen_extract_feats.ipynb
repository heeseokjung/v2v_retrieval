{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/cvpr24_video_retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import ndjson\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from momaapi import MOMA\n",
    "from torchvision import transforms\n",
    "from utils.attr_dict import AttrDict\n",
    "from utils.main_utils import seed_everything\n",
    "from models.frozen import FrozenInTime\n",
    "from utils.clip_sampler import frozen_clip_sampler\n",
    "from utils.frozen_utils import state_dict_data_parallel_fix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/default.yaml\", \"r\") as f:\n",
    "    cfg = yaml.load(f, yaml.FullLoader)\n",
    "    cfg = AttrDict(cfg)\n",
    "\n",
    "if \"SEED\" in cfg:\n",
    "    seed_everything(cfg.SEED)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FrozenInTime(cfg)\n",
    "\n",
    "path = os.path.join(\n",
    "    cfg.PATH.CKPT_PATH, \"FROZEN\", \"cc-webvid2m-4f_stformer_b_16_224.pth.tar\"\n",
    ")\n",
    "assert os.path.isfile(path)\n",
    "sys.path.append(\"utils\")\n",
    "checkpoint = torch.load(path)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "new_state_dict = state_dict_data_parallel_fix(\n",
    "    state_dict, model.state_dict()\n",
    ")\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(snippet):\n",
    "    snippet = torch.from_numpy(snippet)\n",
    "    snippet = snippet.permute(0, 3, 1, 2) # n_frames x 3 x h x w\n",
    "    snippet = snippet / 255.\n",
    "    snippet = transforms.Resize(size=(224, 224))(snippet)\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLIT=(train): 100%|██████████| 904/904 [22:49<00:00,  1.51s/it]\n",
      "SPLIT=(val): 100%|██████████| 226/226 [05:47<00:00,  1.54s/it]\n",
      "SPLIT=(test): 100%|██████████| 282/282 [06:51<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "frozen_args = cfg.MODEL.FROZEN\n",
    "raw_path = \"/data/dir_moma/videos/raw\"\n",
    "feat_path = \"/data/dir_moma/feats/frozen\"\n",
    "clip_duration = frozen_args.clip_duration\n",
    "num_frames = frozen_args.num_frames\n",
    "\n",
    "moma = MOMA(\"/data/dir_moma\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    ids_act = moma.get_ids_act(split=split)\n",
    "    for act in tqdm(moma.get_anns_act(ids_act=ids_act), desc=f\"SPLIT=({split})\"):\n",
    "        if act.id == \"1YzGUyM3P2k\":\n",
    "            continue\n",
    "\n",
    "        video = []\n",
    "        sampled_clips = frozen_clip_sampler(\n",
    "            os.path.join(raw_path, f\"{act.id}.mp4\"), \n",
    "            clip_duration=clip_duration, \n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "    \n",
    "        for clip in sampled_clips:\n",
    "            clip = transform(clip)\n",
    "            video.append(clip)\n",
    "\n",
    "        embeddings = []\n",
    "        for clip in video:\n",
    "            # clip: n_frames(=4) x 3 x 224 x 224\n",
    "            with torch.no_grad():\n",
    "                emb = model(clip.cuda()) # 1 x n_patches x 768\n",
    "            embeddings.append(emb.detach().cpu().numpy())\n",
    "\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        np.save(os.path.join(feat_path, f\"{act.id}.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANIT CHECK\n",
    "missing = []\n",
    "raw_path = \"/data/dir_moma/videos/raw\"\n",
    "feat_path = \"/data/dir_moma/feats/frozen\"\n",
    "for filename in os.listdir(raw_path):\n",
    "    vid = filename[:-4]\n",
    "    if not os.path.exists(os.path.join(feat_path, f\"{vid}.npy\")):\n",
    "        missing.append(vid)\n",
    "\n",
    "print(f\"missing: {missing} ({len(missing)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
